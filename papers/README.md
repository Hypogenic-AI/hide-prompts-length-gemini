# Downloaded Papers

1. [Cognitive Overload Attack: Prompt Injection for Long Context](2410.11277_cognitive_overload.pdf)
   - **arXiv**: 2410.11277
   - **Relevance**: Directly investigates prompt injection in long-context scenarios, proposing a "Cognitive Overload" attack. This is highly relevant to the hypothesis about document length.

2. [Hijacking Large Language Models via Adversarial In-Context Learning](2311.09948_hijacking_llms.pdf)
   - **arXiv**: 2311.09948
   - **Relevance**: Explores adversarial attacks via in-context learning, which is a key mechanism in long-context interactions.

3. [Bypassing LLM Guardrails: An Empirical Analysis of Evasion Attacks](2504.11168_bypassing_guardrails.pdf)
   - **arXiv**: 2504.11168
   - **Relevance**: Recent empirical analysis of evasion attacks, likely covering stealthy methods relevant to hiding prompts.

4. [Human-Interpretable Adversarial Prompt Attack with Situational Context](2407.14644_human_interpretable.pdf)
   - **arXiv**: 2407.14644
   - **Relevance**: Discusses "Situational Context" which implies longer or more complex context, relevant to hiding prompts.

5. [PromptBench: Towards Evaluating the Robustness of LLMs on Adversarial Prompts](2306.04528_promptbench.pdf)
   - **arXiv**: 2306.04528
   - **Relevance**: A benchmark paper providing methodologies and metrics for evaluating adversarial robustness.

6. [Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning](2401.05949_universal_vulnerabilities.pdf)
   - **arXiv**: 2401.05949
   - **Relevance**: Investigates vulnerabilities in ICL, crucial for understanding how hidden prompts might work in long documents.

7. [Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](2302.12173_indirect_prompt_injection.pdf)
   - **arXiv**: 2302.12173
   - **Relevance**: The seminal paper on indirect prompt injection, where prompts are hidden in retrieved documents.
