# Downloaded Papers

1. [Indirect Prompt Injection](2302.12173_indirect_injection.pdf)
   - Authors: Kai Greshake et al.
   - Year: 2023
   - arXiv: 2302.12173
   - Relevance: Foundational paper on indirect prompt injection, where prompts are hidden in external content.

2. [Lost in the Middle: How Language Models Use Long Contexts](2307.03172_lost_in_the_middle.pdf)
   - Authors: Nelson F. Liu et al.
   - Year: 2023
   - arXiv: 2307.03172
   - Relevance: Analyzes how LLM performance degrades in the middle of long contexts, crucial for understanding where to hide prompts.

3. [Jailbreaking Black Box Large Language Models in Twenty Queries](2310.08419_jailbreaking_black_box.pdf)
   - Authors: Patrick Chao et al. (PAIR)
   - Year: 2023
   - arXiv: 2310.08419
   - Relevance: Efficient jailbreaking, relevant for testing attack success rates.

4. [Universal and Transferable Adversarial Attacks on Aligned Language Models](2307.15043_universal_attacks.pdf)
   - Authors: Andy Zou et al. (GCG)
   - Year: 2023
   - arXiv: 2307.15043
   - Relevance: The "GCG" attack, a standard baseline for adversarial suffixes.
